\documentclass{jfm}
\usepackage{graphicx}
\usepackage{epstopdf, epsfig}
\usepackage{xcolor}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\shorttitle{Short Title}
\shortauthor{Short Author}

\title{Title}

\author{Pedro Almeida\aff{1},
  Siddhartha Verma \aff{1,2} \corresp{\email{vermas@fau.edu}}}

\affiliation{\aff{1}Department of Ocean and Mechanical Engineering, Florida Atlantic University, Boca Raton, FL 33431, USA
\aff{2}Harbor Branch Oceanographic Institute, Florida Atlantic University, Fort Pierce, FL 34946, USA}

\begin{document}

\maketitle

%------------------------------------------------------------------------------------------------------------------
% ABSTRACT
%------------------------------------------------------------------------------------------------------------------
\begin{abstract}
ABSTRACT HERE
\end{abstract}


%------------------------------------------------------------------------------------------------------------------
% KEY WORDS
%------------------------------------------------------------------------------------------------------------------
\begin{keywords}
Authors should not enter keywords on the manuscript, as these must be chosen by the author during the online submission process and will then be added during the typesetting process (see http://journals.cambridge.org/data/\linebreak[3]relatedlink/jfm-\linebreak[3]keywords.pdf for the full list)
\end{keywords}


%------------------------------------------------------------------------------------------------------------------
% Introduction
%------------------------------------------------------------------------------------------------------------------
\section{Introduction}
Reinforcement Learning (RL) is a branch of machine learning where an agent learns optimal behavior through repeated interactions with an environment to maximize some notion of a cumulative reward. 

\subsection{RL Model}
\subsubsection{State}
\subsubsection{Action}
\subsubsection{Reward}
\subsubsection{Model-Based vs. Model-Free}

\subsection{Markov Decision Process}
\subsubsection{Full Markov Decision Process}
- Discrete-time stochastic control.

- Future is conditionally independent of past given present $s_t$ and $a_t$.

- Probability of sampling action $a_t$ given state $s_t$.

- Probability of transitioning to state $s_{t+1}$ given state $s_t$ and action $a_t$.

- Calculation of reward based on $s_{t+1}$, $s_t$, and $a_t$.

\subsubsection{Partially Observable Markov Decision Process}

\subsection{Challenges}
 \subsubsection{Exploration vs Exploitation}
- Exploration = Gather info with possibly sub-optimal actions

- Exploitation = Use current knowledge to choose best action

- Short term vs long term

- When to stop exploring

\subsubsection{Temporal Credit Assignment Problem}
- How "good" is an action

- How does it influence the next state / end state

- Which actions lead to desired state

\subsubsection{State Representation}
- Discrete State

- Representative state (CNN)

- Probability Distribution of states

\subsection{Applications}



%------------------------------------------------------------------------------------------------------------------
% Methods
%------------------------------------------------------------------------------------------------------------------
\section{Methods}
\subsection{Q-Learning}
\subsubsection{Theory of Q-Learning}
Q-Learning is a model-free RL algorithm whereby an agent learns the value of an action for a given state  by calculating the expected reward for an action taken in a given state. Originally proposed by Watkins in 1989 \cite{Watkins1989}.

- Works well for discretized environements (like grids)

- Can work with continuous with binning

- Becomes more difficult when size of problem increases (space and time complexity explode)

- Exploring all states in q-learning often takes too long

\begin{equation}
Q^{new}(s_t,a_t) = Q(s_t,a_t) + \alpha \left(r_t +  \gamma Q(s_{t+1},a) - Q(s_t,a_t)\right)
\end{equation}

\subsubsection{Example Environment}
- Grid like 8x8

- One thief (seeking)

- Police Officer (Obstacle)

- Gold (Goal)


\subsection{Deep Q-Learning}
\subsubsection{Deep Q-Learning Theory}
A bit better for continuous state domains

\subsubsection{Example Environment}
\cite{Florian2007}

\cite{1606.01540}

\begin{equation}
\ddot{\theta} = \frac{g sin(\theta) - cos(\theta) \left(\frac{-F - m_{p} L \dot{\theta}^2 sin(\theta)}{m_{t}}\right)}{L * \left[\frac{4}{3} - \frac{m_{p} cos^2(\theta)}{m_{t}}\right]}
\end{equation}

\begin{equation}
\ddot{x} = \frac{F + m_{p}L\left(\dot{\theta}^2 sin(\theta) - \ddot{\theta} cos(\theta) \right)}{m_{t}}
\end{equation}


\subsection{Double Deep Q-Learning}

\subsection{Policy Gradient}

\subsection{Deep Deterministic Policy Gradient}

%------------------------------------------------------------------------------------------------------------------
% Conclusions
%------------------------------------------------------------------------------------------------------------------
\section{Conclusions}


%------------------------------------------------------------------------------------------------------------------
% Bibliography
%------------------------------------------------------------------------------------------------------------------
\bibliographystyle{jfm}
\bibliography{citations}


\end{document}

